{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Third Time's the Charm? StyleGAN3 Inference Notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Environment and Download Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Clone Repo and Install Ninja { display-mode: \"form\" }\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.chdir('/content')\n",
    "CODE_DIR = 'stylegan3-editing'\n",
    "\n",
    "## clone repo\n",
    "!git clone https://github.com/yuval-alaluf/stylegan3-editing $CODE_DIR\n",
    "\n",
    "## install ninja\n",
    "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
    "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
    "\n",
    "## install some packages\n",
    "!pip install pyrallis\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "os.chdir(f'./{CODE_DIR}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Import Packages { display-mode: \"form\" }\n",
    "import time\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import dataclasses\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from editing.interfacegan.face_editor import FaceEditor\n",
    "from editing.styleclip_global_directions import edit as styleclip_edit\n",
    "from models.stylegan3.model import GeneratorType\n",
    "from notebooks.notebook_utils import Downloader, ENCODER_PATHS, INTERFACEGAN_PATHS, STYLECLIP_PATHS\n",
    "from notebooks.notebook_utils import run_alignment, crop_image, compute_transforms\n",
    "from utils.common import tensor2im\n",
    "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Download Configuration\n",
    "Select below whether you wish to download all models using `pydrive`. Note that if you do not use `pydrive`, you may encounter a \"quota exceeded\" error from Google Drive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "download_with_pydrive = True #@param {type:\"boolean\"}\n",
    "downloader = Downloader(code_dir=CODE_DIR,\n",
    "                        use_pydrive=download_with_pydrive,\n",
    "                        subdir=\"pretrained_models\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select Model for Inference\n",
    "Currently, we have ReStyle-pSp and ReStyle-e4e encoders trained for human faces."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Select which model/domain you wish to perform inference on: { display-mode: \"form\" }\n",
    "experiment_type = 'restyle_pSp_ffhq' #@param ['restyle_e4e_ffhq', 'restyle_pSp_ffhq']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Inference Parameters\n",
    "\n",
    "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on. While we provide default values to run this script, feel free to change as needed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EXPERIMENT_DATA_ARGS = {\n",
    "    \"restyle_pSp_ffhq\": {\n",
    "        \"model_path\": \"./pretrained_models/restyle_pSp_ffhq.pt\",\n",
    "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    },\n",
    "    \"restyle_e4e_ffhq\": {\n",
    "        \"model_path\": \"./pretrained_models/restyle_e4e_ffhq.pt\",\n",
    "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
    "        \"transform\": transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "    }\n",
    "}\n",
    "\n",
    "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download Models\n",
    "To reduce the number of requests to fetch the model, we'll check if the model was previously downloaded and saved before downloading the model.\n",
    "We'll download the model for the selected experiment and save it to the folder `stylegan3-editing/pretrained_models`.\n",
    "\n",
    "We also need to verify that the model was downloaded correctly.\n",
    "Note that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Download ReStyle SG3 Encoder { display-mode: \"form\" }\n",
    "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "    print(f'Downloading ReStyle encoder model: {experiment_type}...')\n",
    "    try:\n",
    "      downloader.download_file(file_id=ENCODER_PATHS[experiment_type]['id'],\n",
    "                              file_name=ENCODER_PATHS[experiment_type]['name'])\n",
    "    except Exception as e:\n",
    "      raise ValueError(f\"Unable to download model correctly! {e}\")\n",
    "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
    "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
    "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
    "    else:\n",
    "        print('Done.')\n",
    "else:\n",
    "    print(f'Model for {experiment_type} already exists!')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Pretrained Model\n",
    "We assume that you have downloaded all relevant models and placed them in the directory defined by the\n",
    "`EXPERIMENT_DATA_ARGS` dictionary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Load ReStyle SG3 Encoder { display-mode: \"form\" }\n",
    "model_path = EXPERIMENT_ARGS['model_path']\n",
    "net, opts = load_encoder(checkpoint_path=model_path)\n",
    "pprint.pprint(dataclasses.asdict(opts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare Inputs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Define and Visualize Input { display-mode: \"form\" }\n",
    "\n",
    "image_path = Path(EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"])\n",
    "original_image = Image.open(image_path).convert(\"RGB\")\n",
    "original_image = original_image.resize((256, 256))\n",
    "original_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Get Aligned and Cropped Input Images\n",
    "input_image = run_alignment(image_path)\n",
    "cropped_image = crop_image(image_path)\n",
    "joined = np.concatenate([input_image.resize((256, 256)), cropped_image.resize((256, 256))], axis=1)\n",
    "Image.fromarray(joined)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Compute Landmarks-Based Transforms { display-mode: \"form\" }\n",
    "images_dir = Path(\"./images\")\n",
    "images_dir.mkdir(exist_ok=True, parents=True)\n",
    "cropped_path = images_dir / f\"cropped_{image_path.name}\"\n",
    "aligned_path = images_dir / f\"aligned_{image_path.name}\"\n",
    "cropped_image.save(cropped_path)\n",
    "input_image.save(aligned_path)\n",
    "landmarks_transform = compute_transforms(aligned_path=aligned_path, cropped_path=cropped_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perform Inversion\n",
    "Now we'll run inference. By default, we'll run using 3 inference steps. You can change the parameter in the cell below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title { display-mode: \"form\" }\n",
    "n_iters_per_batch = 3 #@param {type:\"integer\"}\n",
    "opts.n_iters_per_batch = n_iters_per_batch\n",
    "opts.resize_outputs = False  # generate outputs at full resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Run Inference { display-mode: \"form\" }\n",
    "img_transforms = EXPERIMENT_ARGS['transform']\n",
    "transformed_image = img_transforms(input_image)\n",
    "\n",
    "avg_image = get_average_image(net)\n",
    "\n",
    "with torch.no_grad():\n",
    "    tic = time.time()\n",
    "    result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
    "                                                net=net,\n",
    "                                                opts=opts,\n",
    "                                                avg_image=avg_image,\n",
    "                                                landmarks_transform=torch.from_numpy(landmarks_transform).cuda().float())\n",
    "    toc = time.time()\n",
    "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Visualize Result { display-mode: \"form\" }\n",
    "\n",
    "def get_coupled_results(result_batch, cropped_image):\n",
    "    result_tensors = result_batch[0]  # there's one image in our batch\n",
    "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)\n",
    "    final_rec = tensor2im(result_tensors[-1]).resize(resize_amount)\n",
    "    input_im = cropped_image.resize(resize_amount)\n",
    "    res = np.concatenate([np.array(input_im), np.array(final_rec)], axis=1)\n",
    "    res = Image.fromarray(res)\n",
    "    return res\n",
    "\n",
    "res = get_coupled_results(result_batch, cropped_image)\n",
    "res.resize((1024, 512))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Save Result { display-mode: \"form\" }\n",
    "\n",
    "# save image\n",
    "outputs_path = \"./outputs\"\n",
    "os.makedirs(outputs_path, exist_ok=True)\n",
    "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Editing\n",
    "Given the resulting latent code obtained above, we can perform various edits, which we demonstrate below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Download pretrained boundaries and editing files: { display-mode: \"form\" }\n",
    "download_with_pydrive = True #@param {type:\"boolean\"}\n",
    "\n",
    "# download files for interfacegan\n",
    "downloader = Downloader(code_dir=CODE_DIR,\n",
    "                        use_pydrive=download_with_pydrive,\n",
    "                        subdir=\"editing/interfacegan/boundaries/ffhq\")\n",
    "print(\"Downloading InterFaceGAN boundaries...\")\n",
    "for editing_file, params in INTERFACEGAN_PATHS.items():\n",
    "    print(f\"Downloading {editing_file} boundary...\")\n",
    "    downloader.download_file(file_id=params['id'],\n",
    "                             file_name=params['name'])\n",
    "\n",
    "# download files for styleclip\n",
    "downloader = Downloader(code_dir=CODE_DIR,\n",
    "                        use_pydrive=download_with_pydrive,\n",
    "                        subdir=\"editing/styleclip_global_directions/sg3-r-ffhq-1024\")\n",
    "print(\"Downloading StyleCLIP auxiliary files...\")\n",
    "for editing_file, params in STYLECLIP_PATHS.items():\n",
    "    print(f\"Downloading {editing_file}...\")\n",
    "    downloader.download_file(file_id=params['id'],\n",
    "                             file_name=params['name'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### InterFaceGAN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n",
    "\n",
    "#@title Select which edit you wish to perform: { display-mode: \"form\" }\n",
    "edit_direction = 'age' #@param ['age', 'smile', 'pose', 'Male']\n",
    "min_value = -5 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
    "max_value = 5 #@param {type:\"slider\", min:-10, max:10, step:1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Perform Edit! { display-mode: \"form\" }\n",
    "print(f\"Performing edit for {edit_direction}...\")\n",
    "input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
    "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
    "                                        direction=edit_direction,\n",
    "                                        factor_range=(min_value, max_value),\n",
    "                                        user_transforms=landmarks_transform,\n",
    "                                        apply_user_transformations=True)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Show Result { display-mode: \"form\" }\n",
    "def prepare_edited_result(edit_images):\n",
    "  if type(edit_images[0]) == list:\n",
    "      edit_images = [image[0] for image in edit_images]\n",
    "  res = np.array(edit_images[0].resize((512, 512)))\n",
    "  for image in edit_images[1:]:\n",
    "      res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
    "  res = Image.fromarray(res).convert(\"RGB\")\n",
    "  return res\n",
    "\n",
    "res = prepare_edited_result(edit_images)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### StyleCLIP (Global Directions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Prepare StyleCLIP Editor { display-mode: \"form\" }\n",
    "styleclip_args = styleclip_edit.EditConfig()\n",
    "global_direction_calculator = styleclip_edit.load_direction_calculator(stylegan_model=net.decoder, opts=styleclip_args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Select which edit you wish to perform. { display-mode: \"form\" }\n",
    "#@markdown We recommend keeping the neutral_text as is by default.\n",
    "neutral_text = \"a face\" #@param {type:\"raw\"}\n",
    "target_text = \"a smiling face\" #@param {type:\"raw\"}\n",
    "alpha = 4 #@param {type:\"slider\", min:-5, max:5, step:0.5}\n",
    "beta = 0.13 #@param {type:\"slider\", min:-1, max:1, step:0.1}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Perform Edit! { display-mode: \"form\" }\n",
    "opts = styleclip_edit.EditConfig()\n",
    "opts.alpha_min = alpha\n",
    "opts.alpha_max = alpha\n",
    "opts.num_alphas = 1\n",
    "opts.beta_min = beta\n",
    "opts.beta_max = beta\n",
    "opts.num_betas = 1\n",
    "opts.neutral_text = neutral_text\n",
    "opts.target_text = target_text\n",
    "\n",
    "input_latent = result_latents[0][-1]\n",
    "input_transforms = torch.from_numpy(landmarks_transform).cpu().numpy()\n",
    "print(f'Performing edit for: \"{opts.target_text}\"...')\n",
    "edit_res, edit_latent = styleclip_edit.edit_image(latent=input_latent,\n",
    "                                                  landmarks_transform=input_transforms,\n",
    "                                                  stylegan_model=net.decoder,\n",
    "                                                  global_direction_calculator=global_direction_calculator,\n",
    "                                                  opts=opts,\n",
    "                                                  image_name=None,\n",
    "                                                  save=False)\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#@title Show Result { display-mode: \"form\" }\n",
    "input_im = tensor2im(transformed_image).resize((512, 512))\n",
    "edited_im = tensor2im(edit_res[0]).resize((512, 512))\n",
    "edit_coupled = np.concatenate([np.array(input_im), np.array(edited_im)], axis=1)\n",
    "edit_coupled = Image.fromarray(edit_coupled)\n",
    "edit_coupled.resize((1024, 512))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "inference_playground (5).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}